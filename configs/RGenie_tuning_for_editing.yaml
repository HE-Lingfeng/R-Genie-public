wandb:
  entity: null
#  run_id: askkz9i2
  resume: 'auto'

experiment:
    project: "demo"
    name: "RGenie_training"
    output_dir: "RGenie_training"

model:
    RGenie:
        pretrained_model_path: "./hf_model/R-Genie"
        model_path_for_inference: "./runs/RGenie-edit/ckpt_model"
    vq_model:
        type: "magvitv2"
        vq_model_name: "./hf_model/magvitv2"

    showo:
        pretrained_model_path: "./hf_model/show-o"
        model_path_for_inference: "./runs/showo-edit/ckpt_model"
        w_clip_vit: False
        vocab_size: 58498
        llm_vocab_size: 50295
        llm_model_path: './hf_model/phi-1_5'
        codebook_size: 8192
        num_vq_tokens: 256
        num_new_special_tokens: 10  # <|soi|> <|eoi|> <|sov|> <|eov|> <|t2i|> <|mmu|> <|t2v|> <|v2v|> <|lvg|> <|pad|>

    gradient_checkpointing: True

dataset:
    gen_type: "t2i"
    und_type: "captioning"
    data_path: "./data"
    params:
        batch_size: ${training.batch_size}
        shuffle_buffer_size: 1000
        num_workers: 32
        resolution: 256
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 128
        resolution: 256
        center_crop: False
        random_flip: False

training:
    # gradient_accumulation_steps: 1
    cond_dropout_prob: 0.1
    batch_size: 1
    local_rank: 0
    log_base_dir: "./runs"
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.1
    gradient_accumulation_steps: 10
    guidance_scale: 0.0
    generation_timesteps: 12
    generation_temperature: 1.0
    lora_target_modules: "q_proj,v_proj"
    lr: 0.003
    beta1: 0.9
    beta2: 0.95
    epochs: 1 #30
    precision: "bf16"

merge_lora_weight:
    weight: "./runs/showo-edit/pytorch_model.bin/pytorch_model.bin"
    save_path: "./runs/showo-edit/ckpt_model"        

testing:
    test_data_path: "./data"
    save_dir: "./results"

